from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.sessions import SessionMiddleware
from routers import captcha
from typing import List
import os
import httpx
import logging

app = FastAPI()

# ì›ê²© ML ì„œë¹„ìŠ¤ URL
os.environ["REMOTE_ML_SERVICE_URL"] = "http://remote-mlflow-server" # ì‹¤ì œ ì„œë²„ ì£¼ì†Œë¡œ ë³€ê²½

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì™€ í†µì‹  í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # í”„ë¡ íŠ¸ ì£¼ì†Œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„¸ì…˜ ë¯¸ë“¤ì›¨ì–´ (ìº¡ì°¨ í†µê³¼ ì—¬ë¶€ ë“± ì €ì¥ìš©)
app.add_middleware(
    SessionMiddleware,
    secret_key="super-secret-key",
    max_age=3600,  # ì„¸ì…˜ ìœ ì§€ ì‹œê°„ (ì´ˆ)
)

class InferenceRequest(BaseModel):
    inputs: List[List[float]]

@app.get("/")
async def read_root():
    return {"message": "MLflow FastAPI Model Serving Server is running ğŸš€"}

@app.get("/models/")
async def list_models():
    """
    ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
    """
    remote_url = f"{REMOTE_ML_SERVICE_URL}/models/"
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(remote_url)
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logging.error(f"Error listing models: {e}")
        raise HTTPException(status_code=500, detail=f"Error listing models: {e}")

@app.get("/models/{model_name}/versions/")
async def list_model_versions(model_name: str):
    """
    íŠ¹ì • ëª¨ë¸ì˜ ë²„ì „ ëª©ë¡ ë°˜í™˜
    """
    remote_url = f"{REMOTE_ML_SERVICE_URL}/models/{model_name}/versions/"
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(remote_url)
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logging.error(f"Error listing model versions: {e}")
        raise HTTPException(status_code=500, detail=f"Error listing model versions: {e}")

@app.post("/models/predict/{model_name}/", include_in_schema=False)
async def predict_without_version(model_name: str, request: InferenceRequest):
    remote_url = f"{REMOTE_ML_SERVICE_URL}/models/predict/{model_name}/"
    try:
        payload = request.model_dump()
        async with httpx.AsyncClient() as client:
            response = await client.post(remote_url, json=payload)
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logging.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {e}")

@app.post("/models/predict/{model_name}/{version}/", include_in_schema=False)
async def predict_with_version(model_name: str, version: str, request: InferenceRequest):
    remote_url = f"{REMOTE_ML_SERVICE_URL}/models/predict/{model_name}/{version}/"
    try:
        payload = request.model_dump()
        async with httpx.AsyncClient() as client:
            response = await client.post(remote_url, json=payload)
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logging.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {e}")

app.include_router(captcha.router)
