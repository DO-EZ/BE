import logging
import time
from typing import List
from fastapi import APIRouter, HTTPException, Request
from prometheus_client import Counter, Histogram
from pydantic import BaseModel
from src.utils.model_loader import predict_digit
from src.utils.image_processing import decode_image

router = APIRouter(tags=["Model"])

# ==============================
# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
# ==============================
# ì—”ë“œí¬ì¸íŠ¸ë³„ ê¸°ë³¸ ë©”íŠ¸ë¦­
MODEL_REQUEST_COUNT = Counter(
    "model_api_requests_total",
    "Model ê´€ë ¨ API ìš”ì²­ ìˆ˜",
    ["endpoint", "method", "status"],
)
MODEL_REQUEST_LATENCY = Histogram(
    "model_api_request_duration_seconds",
    "Model ê´€ë ¨ API ìš”ì²­ ì²˜ë¦¬ ì‹œê°„(ì´ˆ)",
    ["endpoint", "method"],
)
MODEL_ERROR_COUNT = Counter(
    "model_api_errors_total", "Model ê´€ë ¨ API ì—ëŸ¬ ë°œìƒ ìˆ˜", ["endpoint", "method"]
)
# ==============================

class CaptchaImageRequest(BaseModel):
    id: str
    image: str  # base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´


@router.get("/", summary="ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
async def read_root(request: Request):
    endpoint = "read_root"
    method = request.method
    start_time = time.monotonic()
    status = "200"  # â† ê¸°ë³¸ê°’

    try:
        result = {"message": "MLflow FastAPI Model Serving Server is running ğŸš€"}
        status = "200"
        return result
    finally:
        duration = time.monotonic() - start_time
        MODEL_REQUEST_LATENCY.labels(endpoint=endpoint, method=method).observe(duration)
        MODEL_REQUEST_COUNT.labels(
            endpoint=endpoint, method=method, status=status
        ).inc()

# ë‚´ë¶€ì—ì„œ ëª¨ë¸ ë¡œë“œ í›„ ì¶”ë¡  ìˆ˜í–‰
# ê¸°ì¡´ prometheus ë©”íŠ¸ë¦­ ìœ ì§€
@router.post("/predict", summary="ìº¡ì±  ì´ë¯¸ì§€ë¡œ ì¶”ë¡ ")
async def predict_image(request: Request, body: CaptchaImageRequest):
    endpoint = "predict_image"
    method = request.method
    start_time = time.monotonic()
    status = "200"

    try:
        tensor = decode_image(body.image, captcha_id=body.id)
        prediction = predict_digit(tensor)
        return {"id": body.id, "prediction": prediction}

    except Exception as e:
        status = "500"
        MODEL_ERROR_COUNT.labels(endpoint=endpoint, method=method).inc()
        logging.error(f"ì¶”ë¡  ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì¶”ë¡  ì‹¤íŒ¨: {e}")

    finally:
        duration = time.monotonic() - start_time
        MODEL_REQUEST_LATENCY.labels(endpoint=endpoint, method=method).observe(duration)
        MODEL_REQUEST_COUNT.labels(endpoint=endpoint, method=method, status=status).inc()
